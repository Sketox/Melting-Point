#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¥ğŸ”¥ğŸ”¥ SCRIPT DEFINITIVO - MEGA ENSEMBLE ğŸ”¥ğŸ”¥ğŸ”¥
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OBJETIVO: Bajar de MAE 22.80 â†’ MAE ~10-15

ANÃLISIS DEL PROBLEMA:
- Script nuclear OOF: 28.75 (MALO) porque aÃ±adiÃ³ RF/ET que empeoran
- PASO 6 OOF: 26.64 â†’ Kaggle 22.80 (BUENO)
- Top 1 Kaggle: 4.74

ESTRATEGIA:
1. Mantener features PASO 6 como base (Morgan + MACCS + RDKit + Groups = 2,757)
2. AÃ±adir ChemBERTa embeddings FILTRADOS (no crudos)
3. AÃ±adir descriptores 3D SELECTIVOS (solo los importantes)
4. SMILES Augmentation (5-10x data)
5. Pseudo-labeling con test data
6. Combinar con ChemProp (20-25%)
7. NO usar RF/ET (empeoran)

TÃ‰CNICAS DE GANADORES:
- Knowledge-infused molecular graphs â†’ MAE 10.93 K (paper 2024)
- ChemBERTa + Descriptors â†’ RÂ² > 0.9 (papers)
- SMILES Augmentation â†’ -1 to -2 MAE (comprobado)

Autor: Sketo
Competencia: Kaggle Melting Point
Fecha: Febrero 2026
"""

import os
import sys
import json
import warnings
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import hashlib

import numpy as np
import pandas as pd
from tqdm import tqdm

# Sklearn
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import Ridge
from sklearn.feature_selection import mutual_info_regression

# Gradient Boosting
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor

# RDKit
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors, MACCSkeys
from rdkit import RDLogger
RDLogger.DisableLog('rdApp.*')

warnings.filterwarnings('ignore')
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Detectar paths automÃ¡ticamente
if os.path.exists('/mnt/user-data/MeltingPoint'):
    PROJECT_ROOT = Path('/mnt/user-data/MeltingPoint')
elif os.path.exists('../data/raw'):
    PROJECT_ROOT = Path('..').resolve()
else:
    PROJECT_ROOT = Path('.').resolve()

DATA_RAW = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
SUBMISSIONS_DIR = PROJECT_ROOT / "submissions"

print(f"ğŸ“ PROJECT_ROOT: {PROJECT_ROOT}")

# ConfiguraciÃ³n
N_FOLDS = 5
N_SMILES_AUGMENT = 5  # Variantes SMILES por molÃ©cula
USE_CHEMBERTA = True
USE_3D_DESCRIPTORS = True
USE_SMILES_AUGMENTATION = True
USE_PSEUDO_LABELING = True
CHEMPROP_WEIGHTS = [0.15, 0.20, 0.25, 0.30]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. SMILES AUGMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def randomize_smiles(smiles: str, n_variants: int = 5) -> List[str]:
    """
    Genera n variantes aleatorias de un SMILES.
    Paper: "SMILES Enumeration as Data Augmentation for Neural Network Modeling" (Bjerrum, 2017)
    Mejora tÃ­pica: RÂ² 0.56 â†’ 0.66, RMSE -10%
    """
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [smiles] * n_variants
    
    variants = set([smiles])
    max_attempts = n_variants * 30
    
    for _ in range(max_attempts):
        if len(variants) >= n_variants:
            break
        try:
            random_smi = Chem.MolToSmiles(mol, canonical=False, doRandom=True)
            variants.add(random_smi)
        except:
            pass
    
    variants = list(variants)
    while len(variants) < n_variants:
        variants.append(smiles)
    
    return variants[:n_variants]


def augment_smiles_data(df: pd.DataFrame, 
                        target_col: str = 'Tm',
                        n_augment: int = 5,
                        verbose: bool = True) -> pd.DataFrame:
    """Aumenta dataset con mÃºltiples representaciones SMILES."""
    if verbose:
        print(f"  ğŸ”„ Augmentando datos ({n_augment}x)...")
    
    augmented_rows = []
    iterator = tqdm(df.iterrows(), total=len(df), desc="SMILES Aug") if verbose else df.iterrows()
    
    for _, row in iterator:
        smiles = row['SMILES']
        variants = randomize_smiles(smiles, n_augment)
        
        for variant in variants:
            new_row = row.copy()
            new_row['SMILES'] = variant
            new_row['original_smiles'] = smiles  # Mantener referencia
            augmented_rows.append(new_row)
    
    result = pd.DataFrame(augmented_rows)
    if verbose:
        print(f"  âœ… {len(df)} â†’ {len(result)} muestras ({len(result)/len(df):.1f}x)")
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. CHEMBERTA EMBEDDINGS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ChemBERTaEmbedder:
    """Extrae embeddings de ChemBERTa."""
    
    def __init__(self, model_name: str = "DeepChem/ChemBERTa-77M-MTR"):
        self.model_name = model_name
        self.is_available = False
        
        try:
            import torch
            from transformers import AutoTokenizer, AutoModel
            
            self.torch = torch
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            print(f"  ğŸ¤– Cargando ChemBERTa: {model_name}")
            print(f"     Device: {self.device}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()
            
            self.embedding_dim = self.model.config.hidden_size
            print(f"     Embedding dim: {self.embedding_dim}")
            
            self.is_available = True
            
        except Exception as e:
            print(f"  âš ï¸ ChemBERTa no disponible: {e}")
            self.embedding_dim = 0
    
    def get_embeddings(self, smiles_list: List[str], batch_size: int = 32) -> np.ndarray:
        """Extrae embeddings [CLS] token."""
        if not self.is_available:
            return np.zeros((len(smiles_list), 384))
        
        all_embeddings = []
        
        for i in tqdm(range(0, len(smiles_list), batch_size), desc="ChemBERTa"):
            batch = smiles_list[i:i+batch_size]
            
            inputs = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors="pt"
            ).to(self.device)
            
            with self.torch.no_grad():
                outputs = self.model(**inputs)
                # [CLS] token embedding
                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            all_embeddings.append(embeddings)
        
        return np.vstack(all_embeddings)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. FEATURES (PASO 6 BASE + EXTRAS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_morgan_fingerprints(smiles_list: List[str], 
                            radius: int = 2, 
                            n_bits: int = 2048) -> np.ndarray:
    """Morgan Fingerprints (ECFP4) - 2048 bits."""
    fps = []
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
            fps.append(list(fp))
        else:
            fps.append([0] * n_bits)
    return np.array(fps, dtype=np.float32)


def get_maccs_keys(smiles_list: List[str]) -> np.ndarray:
    """MACCS Keys - 167 bits."""
    fps = []
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol:
            fp = MACCSkeys.GenMACCSKeys(mol)
            fps.append(list(fp))
        else:
            fps.append([0] * 167)
    return np.array(fps, dtype=np.float32)


def get_rdkit_2d_descriptors(smiles_list: List[str]) -> Tuple[np.ndarray, List[str]]:
    """RDKit 2D descriptors - ~25 features."""
    
    desc_funcs = [
        ('MolWt', Descriptors.MolWt),
        ('LogP', Descriptors.MolLogP),
        ('TPSA', Descriptors.TPSA),
        ('NumHDonors', Descriptors.NumHDonors),
        ('NumHAcceptors', Descriptors.NumHAcceptors),
        ('NumRotatableBonds', Descriptors.NumRotatableBonds),
        ('RingCount', Descriptors.RingCount),
        ('NumAromaticRings', Descriptors.NumAromaticRings),
        ('NumSaturatedRings', Descriptors.NumSaturatedRings),
        ('NumAliphaticRings', Descriptors.NumAliphaticRings),
        ('NumHeteroatoms', Descriptors.NumHeteroatoms),
        ('FractionCSP3', Descriptors.FractionCSP3),
        ('HeavyAtomCount', Descriptors.HeavyAtomCount),
        ('LabuteASA', Descriptors.LabuteASA),
        ('BertzCT', Descriptors.BertzCT),
        ('Chi0', Descriptors.Chi0),
        ('Chi1', Descriptors.Chi1),
        ('Kappa1', Descriptors.Kappa1),
        ('Kappa2', Descriptors.Kappa2),
        ('Kappa3', Descriptors.Kappa3),
        ('HallKierAlpha', Descriptors.HallKierAlpha),
        ('MolMR', Descriptors.MolMR),
    ]
    
    features = []
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol:
            feat = []
            for name, func in desc_funcs:
                try:
                    val = func(mol)
                    feat.append(float(val) if val is not None else 0.0)
                except:
                    feat.append(0.0)
            features.append(feat)
        else:
            features.append([0.0] * len(desc_funcs))
    
    names = [name for name, _ in desc_funcs]
    return np.array(features, dtype=np.float32), names


def get_3d_descriptors(smiles_list: List[str]) -> Tuple[np.ndarray, List[str]]:
    """
    3D molecular descriptors - shape, inertia, etc.
    Solo los mÃ¡s importantes para melting point.
    """
    try:
        from rdkit.Chem import Descriptors3D
        has_3d = True
    except:
        has_3d = False
        return np.zeros((len(smiles_list), 0)), []
    
    # Solo descriptores importantes para melting point
    desc_3d_names = [
        'Asphericity', 'Eccentricity', 'InertialShapeFactor',
        'SpherocityIndex', 'RadiusOfGyration',
        'PMI1', 'PMI2', 'PMI3', 'NPR1', 'NPR2'
    ]
    
    features = []
    
    for smi in tqdm(smiles_list, desc="3D Descriptors"):
        mol = Chem.MolFromSmiles(smi)
        feat = [0.0] * len(desc_3d_names)
        
        if mol:
            try:
                mol = Chem.AddHs(mol)
                params = AllChem.ETKDGv3()
                params.randomSeed = RANDOM_STATE
                result = AllChem.EmbedMolecule(mol, params)
                
                if result == 0:
                    try:
                        AllChem.UFFOptimizeMolecule(mol, maxIters=200)
                    except:
                        pass
                    
                    if mol.GetNumConformers() > 0:
                        feat = [
                            Descriptors3D.Asphericity(mol),
                            Descriptors3D.Eccentricity(mol),
                            Descriptors3D.InertialShapeFactor(mol),
                            Descriptors3D.SpherocityIndex(mol),
                            Descriptors3D.RadiusOfGyration(mol),
                            Descriptors3D.PMI1(mol),
                            Descriptors3D.PMI2(mol),
                            Descriptors3D.PMI3(mol),
                            Descriptors3D.NPR1(mol),
                            Descriptors3D.NPR2(mol)
                        ]
            except:
                pass
        
        features.append(feat)
    
    return np.array(features, dtype=np.float32), desc_3d_names


def create_features(smiles_list: List[str],
                    group_features: Optional[np.ndarray] = None,
                    chemberta_embeddings: Optional[np.ndarray] = None,
                    include_3d: bool = False,
                    verbose: bool = True) -> Tuple[np.ndarray, List[str]]:
    """
    Crea features completas (estilo PASO 6 + extras).
    Base: Morgan FP (2048) + MACCS (167) + RDKit 2D (~22) + Groups (~337) = ~2574
    """
    
    all_features = []
    all_names = []
    
    # 1. Morgan FP (2048)
    if verbose:
        print("    â†’ Morgan FP (2048)...")
    morgan = get_morgan_fingerprints(smiles_list)
    all_features.append(morgan)
    all_names.extend([f'morgan_{i}' for i in range(morgan.shape[1])])
    
    # 2. MACCS Keys (167)
    if verbose:
        print("    â†’ MACCS Keys (167)...")
    maccs = get_maccs_keys(smiles_list)
    all_features.append(maccs)
    all_names.extend([f'maccs_{i}' for i in range(maccs.shape[1])])
    
    # 3. RDKit 2D (~22)
    if verbose:
        print("    â†’ RDKit 2D descriptors...")
    rdkit_2d, rdkit_names = get_rdkit_2d_descriptors(smiles_list)
    all_features.append(rdkit_2d)
    all_names.extend([f'rdkit_{n}' for n in rdkit_names])
    
    # 4. 3D Descriptors (opcional, ~10)
    if include_3d:
        if verbose:
            print("    â†’ 3D Descriptors...")
        desc_3d, desc_3d_names = get_3d_descriptors(smiles_list)
        if desc_3d.shape[1] > 0:
            all_features.append(desc_3d)
            all_names.extend([f'3d_{n}' for n in desc_3d_names])
    
    # 5. ChemBERTa embeddings (opcional, 384)
    if chemberta_embeddings is not None and chemberta_embeddings.shape[1] > 0:
        if verbose:
            print(f"    â†’ ChemBERTa ({chemberta_embeddings.shape[1]})...")
        all_features.append(chemberta_embeddings)
        all_names.extend([f'bert_{i}' for i in range(chemberta_embeddings.shape[1])])
    
    # 6. Group features (del dataset)
    if group_features is not None:
        if verbose:
            print(f"    â†’ Group features ({group_features.shape[1]})...")
        all_features.append(group_features)
        all_names.extend([f'group_{i}' for i in range(group_features.shape[1])])
    
    # Concatenar
    X = np.hstack(all_features)
    
    # Limpiar NaN/Inf
    X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)
    
    if verbose:
        print(f"    âœ… Total features: {X.shape[1]}")
    
    return X, all_names


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. MODELOS (Mejores hiperparÃ¡metros del PASO 6)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_optimized_models():
    """
    Modelos con hiperparÃ¡metros optimizados del PASO 6.
    NO incluye RF/ET porque empeoran el ensemble.
    """
    models = {
        'xgb': xgb.XGBRegressor(
            n_estimators=2000,
            max_depth=8,
            learning_rate=0.025,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_lambda=2.5,
            reg_alpha=0.15,
            min_child_weight=4,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            tree_method='hist',
            verbosity=0
        ),
        'lgb': lgb.LGBMRegressor(
            n_estimators=2000,
            max_depth=10,
            num_leaves=80,
            learning_rate=0.025,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_lambda=2.5,
            reg_alpha=0.15,
            min_child_samples=15,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbose=-1
        ),
        'cat': CatBoostRegressor(
            iterations=2000,
            depth=8,
            learning_rate=0.025,
            l2_leaf_reg=4,
            random_seed=RANDOM_STATE,
            verbose=False,
            early_stopping_rounds=100
        )
    }
    return models


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. PSEUDO-LABELING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pseudo_labeling(X_train: np.ndarray, 
                    y_train: np.ndarray,
                    X_test: np.ndarray,
                    test_preds: np.ndarray,
                    confidence_percentile: float = 0.8) -> Tuple[np.ndarray, np.ndarray]:
    """
    Semi-supervised learning: aÃ±ade predicciones de test como pseudo-labels.
    Solo usa las predicciones mÃ¡s confiables (menor varianza entre modelos).
    """
    # Calcular varianza entre modelos si hay mÃºltiples predicciones
    if len(test_preds.shape) > 1:
        variance = np.var(test_preds, axis=0)
        threshold = np.percentile(variance, confidence_percentile * 100)
        confident_mask = variance <= threshold
    else:
        # Si solo hay una predicciÃ³n, usar todas
        confident_mask = np.ones(len(test_preds), dtype=bool)
    
    n_confident = confident_mask.sum()
    print(f"    Pseudo-labels: {n_confident}/{len(test_preds)} muestras confiables")
    
    # Combinar train + pseudo-labeled test
    if len(test_preds.shape) > 1:
        pseudo_y = np.mean(test_preds, axis=0)[confident_mask]
    else:
        pseudo_y = test_preds[confident_mask]
    
    X_combined = np.vstack([X_train, X_test[confident_mask]])
    y_combined = np.concatenate([y_train, pseudo_y])
    
    return X_combined, y_combined


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. PIPELINE PRINCIPAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘     ğŸ”¥ğŸ”¥ğŸ”¥ SCRIPT DEFINITIVO - MEGA ENSEMBLE ğŸ”¥ğŸ”¥ğŸ”¥                         â•‘
â•‘                                                                              â•‘
â•‘     Objetivo: MAE 22.80 â†’ MAE ~15-18                                        â•‘
â•‘                                                                              â•‘
â•‘     TÃ©cnicas:                                                                â•‘
â•‘     âœ“ PASO 6 features como base (Morgan + MACCS + RDKit + Groups)           â•‘
â•‘     âœ“ ChemBERTa embeddings                                                  â•‘
â•‘     âœ“ 3D Molecular Descriptors                                              â•‘
â•‘     âœ“ SMILES Augmentation (5x)                                              â•‘
â•‘     âœ“ XGB + LGB + CAT ensemble (sin RF/ET)                                  â•‘
â•‘     âœ“ CombinaciÃ³n con ChemProp                                              â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # =========================================================================
    # CARGAR DATOS
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ“¥ CARGANDO DATOS")
    print("="*70)
    
    train_df = pd.read_csv(DATA_RAW / "train.csv")
    test_df = pd.read_csv(DATA_RAW / "test.csv")
    
    print(f"  Train: {len(train_df)} muestras")
    print(f"  Test:  {len(test_df)} muestras")
    
    y = train_df['Tm'].values
    print(f"  Target range: {y.min():.1f} - {y.max():.1f} K")
    
    # Group features
    group_cols = [c for c in train_df.columns if c.startswith('Group')]
    train_groups = train_df[group_cols].values.astype(np.float32)
    test_groups = test_df[group_cols].values.astype(np.float32)
    print(f"  Group features: {len(group_cols)}")
    
    # =========================================================================
    # CHEMBERTA EMBEDDINGS
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ¤– CHEMBERTA EMBEDDINGS")
    print("="*70)
    
    if USE_CHEMBERTA:
        chemberta = ChemBERTaEmbedder()
        
        if chemberta.is_available:
            print("\n  Extrayendo embeddings para train...")
            train_bert = chemberta.get_embeddings(train_df['SMILES'].tolist())
            
            print("  Extrayendo embeddings para test...")
            test_bert = chemberta.get_embeddings(test_df['SMILES'].tolist())
        else:
            train_bert = None
            test_bert = None
    else:
        train_bert = None
        test_bert = None
    
    # =========================================================================
    # CREAR FEATURES
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ”§ FEATURE ENGINEERING")
    print("="*70)
    
    print("\n  Creando features para TRAIN:")
    X_train, feature_names = create_features(
        train_df['SMILES'].tolist(),
        group_features=train_groups,
        chemberta_embeddings=train_bert,
        include_3d=USE_3D_DESCRIPTORS
    )
    
    print("\n  Creando features para TEST:")
    X_test, _ = create_features(
        test_df['SMILES'].tolist(),
        group_features=test_groups,
        chemberta_embeddings=test_bert,
        include_3d=USE_3D_DESCRIPTORS
    )
    
    # =========================================================================
    # CROSS-VALIDATION
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ‹ï¸ CROSS-VALIDATION ({} FOLDS)".format(N_FOLDS))
    print("="*70)
    
    kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    
    model_names = ['xgb', 'lgb', 'cat']
    all_models = {name: [] for name in model_names}
    scalers = []
    
    oof_preds = {name: np.zeros(len(train_df)) for name in model_names}
    test_preds = {name: [] for name in model_names}
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):
        print(f"\n  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"  FOLD {fold+1}/{N_FOLDS}")
        print(f"  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # Split
        X_fold_train = X_train[train_idx]
        X_fold_val = X_train[val_idx]
        y_fold_train = y[train_idx]
        y_fold_val = y[val_idx]
        
        # SMILES Augmentation en training
        if USE_SMILES_AUGMENTATION and fold == 0:  # Solo primer fold para ahorrar tiempo
            print(f"\n    ğŸ“Š Aplicando SMILES Augmentation...")
            
            fold_train_df = train_df.iloc[train_idx].copy()
            fold_train_aug = augment_smiles_data(fold_train_df, n_augment=N_SMILES_AUGMENT, verbose=True)
            
            # Recalcular features para datos augmentados
            print("    Recalculando features para datos augmentados...")
            aug_groups = np.tile(train_groups[train_idx], (N_SMILES_AUGMENT, 1))
            aug_bert = np.tile(train_bert[train_idx], (N_SMILES_AUGMENT, 1)) if train_bert is not None else None
            
            # IMPORTANTE: Usar include_3d=True para mantener misma dimensiÃ³n
            # Pero para ahorrar tiempo, replicamos los 3D features del original
            X_fold_train_aug_base, _ = create_features(
                fold_train_aug['SMILES'].tolist(),
                group_features=aug_groups,
                chemberta_embeddings=aug_bert,
                include_3d=False,  # Skip 3D calculation (muy lento)
                verbose=False
            )
            
            # AÃ±adir columnas de 3D como ceros (placeholder) para mantener dimensiÃ³n
            n_3d_features = X_train.shape[1] - X_fold_train_aug_base.shape[1]
            if n_3d_features > 0:
                zeros_3d = np.zeros((X_fold_train_aug_base.shape[0], n_3d_features), dtype=np.float32)
                X_fold_train_aug = np.hstack([X_fold_train_aug_base, zeros_3d])
            else:
                X_fold_train_aug = X_fold_train_aug_base
            
            y_fold_train_aug = fold_train_aug['Tm'].values
            
            # Usar datos augmentados solo para XGB (el mÃ¡s robusto)
            X_fold_train_xgb = X_fold_train_aug
            y_fold_train_xgb = y_fold_train_aug
            
            print(f"    Features augmentados: {X_fold_train_aug.shape[1]} (matched)")
        else:
            X_fold_train_xgb = X_fold_train
            y_fold_train_xgb = y_fold_train
        
        # Escalar
        scaler = StandardScaler()
        X_fold_train_s = scaler.fit_transform(X_fold_train)
        X_fold_val_s = scaler.transform(X_fold_val)
        X_test_s = scaler.transform(X_test)
        scalers.append(scaler)
        
        if USE_SMILES_AUGMENTATION and fold == 0:
            X_fold_train_xgb_s = scaler.transform(X_fold_train_xgb)
        else:
            X_fold_train_xgb_s = X_fold_train_s
        
        # Entrenar modelos
        models = get_optimized_models()
        
        for name in model_names:
            model = models[name]
            
            # Usar datos augmentados solo para XGB en fold 0
            if name == 'xgb' and USE_SMILES_AUGMENTATION and fold == 0:
                X_tr, y_tr = X_fold_train_xgb_s, y_fold_train_xgb
            else:
                X_tr, y_tr = X_fold_train_s, y_fold_train
            
            print(f"    Training {name}...", end=" ")
            
            if name == 'cat':
                model.fit(X_tr, y_tr, 
                         eval_set=(X_fold_val_s, y_fold_val),
                         verbose=False)
            elif name == 'lgb':
                model.fit(X_tr, y_tr,
                         eval_set=[(X_fold_val_s, y_fold_val)])
            else:
                model.fit(X_tr, y_tr,
                         eval_set=[(X_fold_val_s, y_fold_val)],
                         verbose=False)
            
            all_models[name].append(model)
            
            # Predicciones
            val_pred = model.predict(X_fold_val_s)
            oof_preds[name][val_idx] = val_pred
            test_preds[name].append(model.predict(X_test_s))
            
            mae = mean_absolute_error(y_fold_val, val_pred)
            print(f"Val MAE = {mae:.2f}")
    
    # Promediar predicciones de test
    test_preds_avg = {name: np.mean(test_preds[name], axis=0) for name in model_names}
    
    # =========================================================================
    # STACKING
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ—ï¸ STACKING (Meta-Learner)")
    print("="*70)
    
    # Meta-features
    meta_train = np.column_stack([oof_preds[name] for name in model_names])
    meta_test = np.column_stack([test_preds_avg[name] for name in model_names])
    
    # Meta-learner
    meta_model = Ridge(alpha=1.0)
    meta_model.fit(meta_train, y)
    
    stacked_oof = meta_model.predict(meta_train)
    stacked_test = meta_model.predict(meta_test)
    
    stacked_mae = mean_absolute_error(y, stacked_oof)
    
    print(f"\n  Meta-learner weights:")
    for i, name in enumerate(model_names):
        print(f"    {name}: {meta_model.coef_[i]:.3f}")
    
    print(f"\n  ğŸ“Š OOF Results:")
    for name in model_names:
        mae = mean_absolute_error(y, oof_preds[name])
        print(f"    {name}: {mae:.2f}")
    print(f"    Stacked: {stacked_mae:.2f}")
    
    # =========================================================================
    # COMBINAR CON CHEMPROP
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ”— COMBINANDO CON CHEMPROP")
    print("="*70)
    
    cp_path = DATA_PROCESSED / "chemprop_predictions.csv"
    
    if cp_path.exists():
        cp_df = pd.read_csv(cp_path)
        cp_test = cp_df['Tm'].values if 'Tm' in cp_df.columns else cp_df.iloc[:, 0].values
        print(f"  âœ… ChemProp cargado: {len(cp_test)} predicciones")
    else:
        cp_test = None
        print("  âš ï¸ ChemProp no encontrado en:", cp_path)
    
    # =========================================================================
    # GENERAR SUBMISSIONS
    # =========================================================================
    
    print("\n" + "="*70)
    print("  ğŸ“ GENERANDO SUBMISSIONS")
    print("="*70)
    
    SUBMISSIONS_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    
    submissions = []
    
    # 1. Solo Stacked (baseline)
    sub = pd.DataFrame({'id': test_df['id'], 'Tm': stacked_test})
    filename = f"definitivo_stacked_{timestamp}.csv"
    sub.to_csv(SUBMISSIONS_DIR / filename, index=False)
    submissions.append(('stacked', stacked_mae, filename))
    print(f"  âœ… {filename}")
    
    # 2-5. Con diferentes pesos de ChemProp
    if cp_test is not None:
        for w_cp in CHEMPROP_WEIGHTS:
            combined = w_cp * cp_test + (1 - w_cp) * stacked_test
            filename = f"definitivo_cp{int(w_cp*100)}_{timestamp}.csv"
            pd.DataFrame({'id': test_df['id'], 'Tm': combined}).to_csv(
                SUBMISSIONS_DIR / filename, index=False
            )
            submissions.append((f'cp{int(w_cp*100)}', None, filename))
            print(f"  âœ… {filename}")
    
    # =========================================================================
    # RESUMEN FINAL
    # =========================================================================
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  âœ… SCRIPT DEFINITIVO COMPLETADO                                             â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ğŸ“Š OOF MAE RESULTS:                                                        â•‘
â•‘      â€¢ XGBoost:   {mean_absolute_error(y, oof_preds['xgb']):>6.2f}                                              â•‘
â•‘      â€¢ LightGBM:  {mean_absolute_error(y, oof_preds['lgb']):>6.2f}                                              â•‘
â•‘      â€¢ CatBoost:  {mean_absolute_error(y, oof_preds['cat']):>6.2f}                                              â•‘
â•‘      â€¢ Stacked:   {stacked_mae:>6.2f}                                              â•‘
â•‘                                                                              â•‘
â•‘  ğŸ“ˆ Features utilizadas: {X_train.shape[1]:>5}                                          â•‘
â•‘      â€¢ Morgan FP:    2,048                                                   â•‘
â•‘      â€¢ MACCS Keys:     167                                                   â•‘
â•‘      â€¢ RDKit 2D:       ~22                                                   â•‘
â•‘      â€¢ 3D Descriptors: ~10                                                   â•‘
â•‘      â€¢ ChemBERTa:      384                                                   â•‘
â•‘      â€¢ Group features: {len(group_cols):>3}                                                   â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ğŸ¯ RECOMENDACIÃ“N DE SUBMISSION (basado en PASO 6):                         â•‘
â•‘                                                                              â•‘
â•‘      1. definitivo_cp20 (20% ChemProp) - RECOMENDADO                        â•‘
â•‘      2. definitivo_cp25 (25% ChemProp) - Segunda opciÃ³n                     â•‘
â•‘      3. definitivo_cp15 (15% ChemProp) - MÃ¡s conservador                    â•‘
â•‘                                                                              â•‘
â•‘  ğŸ“Š COMPARACIÃ“N vs PASO 6:                                                  â•‘
â•‘      â€¢ PASO 6 OOF: 26.64 â†’ Kaggle 22.80                                     â•‘
â•‘      â€¢ Actual OOF: {stacked_mae:.2f} â†’ Kaggle ???                                      â•‘
â•‘                                                                              â•‘
â•‘      Si OOF < 26.64 â†’ Probablemente mejor en Kaggle                         â•‘
â•‘      Si OOF > 26.64 â†’ Probablemente peor (NO subir)                         â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Guardar configuraciÃ³n
    config = {
        'timestamp': timestamp,
        'n_folds': N_FOLDS,
        'n_features': int(X_train.shape[1]),
        'use_chemberta': USE_CHEMBERTA,
        'use_3d': USE_3D_DESCRIPTORS,
        'use_augmentation': USE_SMILES_AUGMENTATION,
        'n_smiles_augment': N_SMILES_AUGMENT,
        'model_oof_maes': {
            name: float(mean_absolute_error(y, oof_preds[name])) 
            for name in model_names
        },
        'stacked_oof_mae': float(stacked_mae),
        'meta_weights': {
            name: float(meta_model.coef_[i]) 
            for i, name in enumerate(model_names)
        },
        'submissions': [
            {'name': name, 'oof_mae': mae, 'file': filename}
            for name, mae, filename in submissions
        ]
    }
    
    config_path = SUBMISSIONS_DIR / f"config_definitivo_{timestamp}.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"\n  ğŸ’¾ Config guardada: {config_path}")
    
    return config


if __name__ == "__main__":
    config = main()