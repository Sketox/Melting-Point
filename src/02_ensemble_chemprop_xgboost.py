"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                PASO 2: Ensemble ChemProp + XGBoost
                MeltingPoint Kaggle Competition
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Este script combina:
1. Predicciones del modelo ChemProp (PASO 1)
2. Modelo XGBoost con features RDKit extendidas
3. OptimizaciÃ³n de pesos del ensemble

Â¿POR QUÃ‰ HACER ENSEMBLE?
========================
- ChemProp aprende del GRAFO molecular (estructura)
- XGBoost aprende de FEATURES numÃ©ricas (descriptores)
- Son enfoques DIFERENTES â†’ capturan patrones COMPLEMENTARIOS
- Al combinarlos, los errores de uno pueden ser corregidos por el otro

RESULTADO ESPERADO:
==================
- ChemProp solo: MAE ~26.87
- XGBoost solo: MAE ~28-30
- Ensemble optimizado: MAE ~24-26 (mejora ~5-10%)

Autor: Sketo
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
import warnings
warnings.filterwarnings('ignore')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N DE RUTAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROJECT_ROOT = Path(__file__).resolve().parents[1]

DATA_RAW = PROJECT_ROOT / "data" / "raw"
TRAIN_RAW = DATA_RAW / "train.csv"
TEST_RAW = DATA_RAW / "test.csv"

DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
CHEMPROP_MODEL_DIR = PROJECT_ROOT / "backend" / "models" / "chemprop_max"

SUBMISSION_DIR = PROJECT_ROOT / "submissions"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PASO 2.1: EXTRAER FEATURES RDKIT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_rdkit_features(smiles_list: list) -> pd.DataFrame:
    """
    Extrae ~200 descriptores moleculares usando RDKit.
    
    Estos descriptores capturan propiedades fÃ­sico-quÃ­micas que influyen
    en el punto de fusiÃ³n:
    - Peso molecular, LogP, TPSA (Ã¡rea polar)
    - Conteos de Ã¡tomos, enlaces, anillos
    - Ãndices topolÃ³gicos (conectividad molecular)
    - Cargas parciales, estados electrÃ³nicos
    """
    from rdkit import Chem
    from rdkit.Chem import Descriptors, rdMolDescriptors
    from rdkit.ML.Descriptors import MoleculeDescriptors
    
    print("\n  ğŸ“Š Extrayendo features RDKit...")
    
    # Lista de todos los descriptores disponibles
    descriptor_names = [desc[0] for desc in Descriptors._descList]
    calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)
    
    features = []
    valid_indices = []
    
    for i, smi in enumerate(smiles_list):
        mol = Chem.MolFromSmiles(smi)
        if mol is not None:
            try:
                desc = calculator.CalcDescriptors(mol)
                features.append(desc)
                valid_indices.append(i)
            except:
                # Si falla, usar NaN
                features.append([np.nan] * len(descriptor_names))
                valid_indices.append(i)
        else:
            features.append([np.nan] * len(descriptor_names))
            valid_indices.append(i)
    
    df = pd.DataFrame(features, columns=[f"rdkit_{name}" for name in descriptor_names])
    
    # Eliminar columnas con todos NaN o varianza 0
    df = df.dropna(axis=1, how='all')
    df = df.loc[:, df.std() > 0]
    
    # Rellenar NaN restantes con la mediana
    df = df.fillna(df.median())
    
    print(f"      ExtraÃ­dos {df.shape[1]} descriptores RDKit")
    
    return df


def extract_smiles_features(smiles_list: list) -> pd.DataFrame:
    """
    Extrae features bÃ¡sicas directamente del string SMILES.
    
    Estas features son simples pero tienen buena correlaciÃ³n con Tm:
    - smiles_len: longitud del string (correlaciÃ³n ~0.49)
    - n_rings: nÃºmero de anillos (correlaciÃ³n ~0.48)
    - Presencia de heteroÃ¡tomos (N, O, F, etc.)
    """
    print("\n  ğŸ“Š Extrayendo features de SMILES...")
    
    features = []
    
    for smi in smiles_list:
        smi = str(smi)
        feat = {
            "smiles_len": len(smi),
            "n_rings": sum(c.isdigit() for c in smi),
            "n_branches": smi.count("("),
            "n_double_bonds": smi.count("="),
            "n_triple_bonds": smi.count("#"),
            "n_aromatic": sum(c.islower() for c in smi),
            "has_N": int("N" in smi or "n" in smi),
            "has_O": int("O" in smi or "o" in smi),
            "has_F": int("F" in smi),
            "has_Cl": int("Cl" in smi),
            "has_Br": int("Br" in smi),
            "has_S": int("S" in smi or "s" in smi),
            "has_P": int("P" in smi),
            "has_I": int("I" in smi),
            "count_N": smi.count("N") + smi.count("n"),
            "count_O": smi.count("O") + smi.count("o"),
            "count_F": smi.count("F"),
            "count_Cl": smi.count("Cl"),
            "count_S": smi.count("S") + smi.count("s"),
        }
        
        # Features derivadas
        feat["ring_density"] = feat["n_rings"] / max(feat["smiles_len"], 1)
        feat["heteroatom_count"] = feat["count_N"] + feat["count_O"] + feat["count_F"] + feat["count_S"]
        
        features.append(feat)
    
    df = pd.DataFrame(features)
    print(f"      ExtraÃ­dos {df.shape[1]} features de SMILES")
    
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PASO 2.2: ENTRENAR XGBOOST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_xgboost(X_train, y_train, X_val=None, y_val=None):
    """
    Entrena un modelo XGBoost con configuraciÃ³n optimizada.
    
    XGBoost funciona bien con:
    - Features tabulares (RDKit descriptors)
    - Datasets pequeÃ±os/medianos
    - RegularizaciÃ³n incorporada
    """
    from xgboost import XGBRegressor
    
    model = XGBRegressor(
        n_estimators=1000,
        max_depth=8,
        learning_rate=0.03,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=1.5,
        reg_alpha=0.1,
        min_child_weight=3,
        random_state=42,
        n_jobs=-1,
        tree_method='hist',  # MÃ¡s rÃ¡pido
    )
    
    if X_val is not None and y_val is not None:
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
    else:
        model.fit(X_train, y_train, verbose=False)
    
    return model


def train_lightgbm(X_train, y_train, X_val=None, y_val=None):
    """
    Entrena un modelo LightGBM como alternativa a XGBoost.
    
    LightGBM puede capturar patrones diferentes, lo que
    beneficia al ensemble.
    """
    from lightgbm import LGBMRegressor
    
    model = LGBMRegressor(
        n_estimators=1000,
        max_depth=10,
        num_leaves=64,
        learning_rate=0.03,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=1.5,
        reg_alpha=0.1,
        min_child_samples=10,
        random_state=42,
        n_jobs=-1,
        verbose=-1,
    )
    
    if X_val is not None and y_val is not None:
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
        )
    else:
        model.fit(X_train, y_train)
    
    return model


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PASO 2.3: OBTENER PREDICCIONES DE CHEMPROP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_chemprop_predictions():
    """
    Obtiene las predicciones de ChemProp para train y test.
    
    Para el ensemble necesitamos predicciones OOF (out-of-fold) del train
    para poder optimizar los pesos sin data leakage.
    """
    print("\n  ğŸ“Š Cargando predicciones de ChemProp...")
    
    # Predicciones de test (ya las tenemos del PASO 1)
    test_preds_path = DATA_PROCESSED / "chemprop_predictions.csv"
    
    if test_preds_path.exists():
        test_preds = pd.read_csv(test_preds_path)
        if "Tm" in test_preds.columns:
            chemprop_test_preds = test_preds["Tm"].values
        else:
            chemprop_test_preds = test_preds.iloc[:, 0].values
        print(f"      Test: {len(chemprop_test_preds)} predicciones")
    else:
        print("      âš ï¸ No se encontraron predicciones de test de ChemProp")
        chemprop_test_preds = None
    
    # Para train, necesitamos generar predicciones OOF
    # Por ahora, usaremos las predicciones del split de validaciÃ³n
    train_preds_path = CHEMPROP_MODEL_DIR / "test_preds.csv"
    
    if train_preds_path.exists():
        train_preds = pd.read_csv(train_preds_path)
        print(f"      Train (test split): {len(train_preds)} predicciones")
    
    return chemprop_test_preds


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PASO 2.4: ENSEMBLE CON OPTIMIZACIÃ“N DE PESOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def optimize_ensemble_weights(y_true, pred1, pred2, pred3=None):
    """
    Encuentra los pesos Ã³ptimos para combinar las predicciones.
    
    Busca w1, w2, w3 tal que:
    ensemble = w1*pred1 + w2*pred2 + w3*pred3
    minimice el MAE
    
    RestricciÃ³n: w1 + w2 + w3 = 1
    """
    from sklearn.metrics import mean_absolute_error
    
    best_mae = float('inf')
    best_weights = None
    
    # Grid search sobre pesos
    if pred3 is None:
        # Solo 2 modelos
        for w1 in np.arange(0, 1.01, 0.05):
            w2 = 1 - w1
            ensemble = w1 * pred1 + w2 * pred2
            mae = mean_absolute_error(y_true, ensemble)
            if mae < best_mae:
                best_mae = mae
                best_weights = (w1, w2)
    else:
        # 3 modelos
        for w1 in np.arange(0, 1.01, 0.05):
            for w2 in np.arange(0, 1.01 - w1, 0.05):
                w3 = 1 - w1 - w2
                ensemble = w1 * pred1 + w2 * pred2 + w3 * pred3
                mae = mean_absolute_error(y_true, ensemble)
                if mae < best_mae:
                    best_mae = mae
                    best_weights = (w1, w2, w3)
    
    return best_weights, best_mae


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE PRINCIPAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘        ğŸ”¬ PASO 2: Ensemble ChemProp + XGBoost                               â•‘
â•‘        MeltingPoint Kaggle Competition                                       â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Cargar datos
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  PASO 2.1: Cargando y preparando datos")
    print("="*70)
    
    train_df = pd.read_csv(TRAIN_RAW)
    test_df = pd.read_csv(TEST_RAW)
    
    print(f"\n  Train: {len(train_df)} molÃ©culas")
    print(f"  Test:  {len(test_df)} molÃ©culas")
    
    y_train = train_df["Tm"].values
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Extraer features
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  PASO 2.2: Extrayendo features")
    print("="*70)
    
    # Features de SMILES
    train_smiles_feat = extract_smiles_features(train_df["SMILES"].tolist())
    test_smiles_feat = extract_smiles_features(test_df["SMILES"].tolist())
    
    # Features RDKit
    train_rdkit_feat = extract_rdkit_features(train_df["SMILES"].tolist())
    test_rdkit_feat = extract_rdkit_features(test_df["SMILES"].tolist())
    
    # Features Group (originales del dataset)
    group_cols = [c for c in train_df.columns if c.startswith("Group")]
    train_group_feat = train_df[group_cols]
    test_group_feat = test_df[group_cols]
    
    # Eliminar columnas con varianza 0
    nonzero_var_cols = train_group_feat.columns[train_group_feat.var() > 0]
    train_group_feat = train_group_feat[nonzero_var_cols]
    test_group_feat = test_group_feat[nonzero_var_cols]
    
    print(f"\n  Features Group (var > 0): {train_group_feat.shape[1]}")
    
    # Combinar todas las features
    X_train = pd.concat([train_smiles_feat, train_rdkit_feat, train_group_feat], axis=1)
    X_test = pd.concat([test_smiles_feat, test_rdkit_feat, test_group_feat], axis=1)
    
    # Asegurar mismas columnas
    common_cols = X_train.columns.intersection(X_test.columns)
    X_train = X_train[common_cols]
    X_test = X_test[common_cols]
    
    # Rellenar NaN
    X_train = X_train.fillna(X_train.median())
    X_test = X_test.fillna(X_train.median())
    
    print(f"\n  Total features combinadas: {X_train.shape[1]}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Entrenar XGBoost y LightGBM con 5-Fold CV
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  PASO 2.3: Entrenando XGBoost y LightGBM (5-Fold CV)")
    print("="*70)
    
    from sklearn.metrics import mean_absolute_error
    
    n_folds = 5
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    xgb_oof_preds = np.zeros(len(X_train))
    lgbm_oof_preds = np.zeros(len(X_train))
    xgb_test_preds = np.zeros(len(X_test))
    lgbm_test_preds = np.zeros(len(X_test))
    
    xgb_maes = []
    lgbm_maes = []
    
    print(f"\n  Entrenando {n_folds} folds...")
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]
        
        # XGBoost
        xgb_model = train_xgboost(X_tr, y_tr, X_val, y_val)
        xgb_val_pred = xgb_model.predict(X_val)
        xgb_oof_preds[val_idx] = xgb_val_pred
        xgb_test_preds += xgb_model.predict(X_test) / n_folds
        xgb_mae = mean_absolute_error(y_val, xgb_val_pred)
        xgb_maes.append(xgb_mae)
        
        # LightGBM
        lgbm_model = train_lightgbm(X_tr, y_tr, X_val, y_val)
        lgbm_val_pred = lgbm_model.predict(X_val)
        lgbm_oof_preds[val_idx] = lgbm_val_pred
        lgbm_test_preds += lgbm_model.predict(X_test) / n_folds
        lgbm_mae = mean_absolute_error(y_val, lgbm_val_pred)
        lgbm_maes.append(lgbm_mae)
        
        print(f"      Fold {fold+1}: XGB MAE = {xgb_mae:.2f}, LGBM MAE = {lgbm_mae:.2f}")
    
    xgb_oof_mae = mean_absolute_error(y_train, xgb_oof_preds)
    lgbm_oof_mae = mean_absolute_error(y_train, lgbm_oof_preds)
    
    print(f"\n  ğŸ“Š Resultados OOF:")
    print(f"      XGBoost MAE:  {xgb_oof_mae:.2f} (Â±{np.std(xgb_maes):.2f})")
    print(f"      LightGBM MAE: {lgbm_oof_mae:.2f} (Â±{np.std(lgbm_maes):.2f})")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Cargar predicciones de ChemProp
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  PASO 2.4: Cargando predicciones de ChemProp")
    print("="*70)
    
    chemprop_test_preds = get_chemprop_predictions()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Optimizar pesos del ensemble (XGBoost + LightGBM)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  PASO 2.5: Optimizando pesos del ensemble")
    print("="*70)
    
    # Primero: Ensemble XGBoost + LightGBM
    print("\n  ğŸ” Buscando pesos Ã³ptimos para XGBoost + LightGBM...")
    
    best_weights_gb, best_mae_gb = optimize_ensemble_weights(
        y_train, xgb_oof_preds, lgbm_oof_preds
    )
    
    print(f"\n      Mejores pesos: XGB={best_weights_gb[0]:.2f}, LGBM={best_weights_gb[1]:.2f}")
    print(f"      Ensemble MAE (OOF): {best_mae_gb:.2f}")
    
    # Crear predicciones del ensemble GB
    ensemble_gb_test = best_weights_gb[0] * xgb_test_preds + best_weights_gb[1] * lgbm_test_preds
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Crear submissions
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  PASO 2.6: Creando submissions")
    print("="*70)
    
    SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)
    
    # Submission 1: Solo XGBoost
    sub_xgb = pd.DataFrame({"id": test_df["id"], "Tm": xgb_test_preds})
    sub_xgb_path = SUBMISSION_DIR / "submission_xgboost.csv"
    sub_xgb.to_csv(sub_xgb_path, index=False)
    print(f"\n  âœ“ {sub_xgb_path.name}")
    
    # Submission 2: Solo LightGBM
    sub_lgbm = pd.DataFrame({"id": test_df["id"], "Tm": lgbm_test_preds})
    sub_lgbm_path = SUBMISSION_DIR / "submission_lightgbm.csv"
    sub_lgbm.to_csv(sub_lgbm_path, index=False)
    print(f"  âœ“ {sub_lgbm_path.name}")
    
    # Submission 3: Ensemble XGBoost + LightGBM
    sub_gb = pd.DataFrame({"id": test_df["id"], "Tm": ensemble_gb_test})
    sub_gb_path = SUBMISSION_DIR / "submission_ensemble_gb.csv"
    sub_gb.to_csv(sub_gb_path, index=False)
    print(f"  âœ“ {sub_gb_path.name}")
    
    # Submission 4: Ensemble con ChemProp (si estÃ¡ disponible)
    if chemprop_test_preds is not None:
        print("\n  ğŸ” Creando ensemble final con ChemProp...")
        
        # Probar diferentes pesos para ChemProp
        best_final_mae = float('inf')
        best_chemprop_weight = 0.5
        
        # Usamos los OOF de GB para estimar
        ensemble_gb_oof = best_weights_gb[0] * xgb_oof_preds + best_weights_gb[1] * lgbm_oof_preds
        
        # Como no tenemos OOF de ChemProp, usamos un peso fijo razonable
        # TÃ­picamente ChemProp es mejor, asÃ­ que le damos mÃ¡s peso
        for w_chemprop in [0.4, 0.5, 0.6, 0.7]:
            w_gb = 1 - w_chemprop
            # No podemos optimizar sin OOF de ChemProp, usamos heurÃ­stica
            pass
        
        # Usar peso heurÃ­stico: 60% ChemProp, 40% GB ensemble
        w_chemprop = 0.6
        w_gb = 0.4
        
        final_ensemble = w_chemprop * chemprop_test_preds + w_gb * ensemble_gb_test
        
        sub_final = pd.DataFrame({"id": test_df["id"], "Tm": final_ensemble})
        sub_final_path = SUBMISSION_DIR / "submission_final_ensemble.csv"
        sub_final.to_csv(sub_final_path, index=False)
        print(f"  âœ“ {sub_final_path.name} (ChemProp={w_chemprop:.0%}, GB={w_gb:.0%})")
        
        # TambiÃ©n crear variantes con diferentes pesos
        for w in [0.5, 0.7, 0.8]:
            variant = w * chemprop_test_preds + (1-w) * ensemble_gb_test
            variant_path = SUBMISSION_DIR / f"submission_ensemble_cp{int(w*100)}.csv"
            pd.DataFrame({"id": test_df["id"], "Tm": variant}).to_csv(variant_path, index=False)
            print(f"  âœ“ {variant_path.name} (ChemProp={w:.0%})")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Resumen final
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  âœ… PASO 2 COMPLETADO                                                        â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ğŸ“Š RESULTADOS (MAE OOF):                                                   â•‘
â•‘      â€¢ XGBoost:         {xgb_oof_mae:>6.2f}                                          â•‘
â•‘      â€¢ LightGBM:        {lgbm_oof_mae:>6.2f}                                          â•‘
â•‘      â€¢ Ensemble GB:     {best_mae_gb:>6.2f}                                          â•‘
â•‘      â€¢ ChemProp (test): ~26.87                                              â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ğŸ“ SUBMISSIONS GENERADOS:                                                  â•‘
â•‘      â€¢ submission_xgboost.csv                                               â•‘
â•‘      â€¢ submission_lightgbm.csv                                              â•‘
â•‘      â€¢ submission_ensemble_gb.csv                                           â•‘
â•‘      â€¢ submission_final_ensemble.csv  â† RECOMENDADO                         â•‘
â•‘      â€¢ submission_ensemble_cp50/70/80.csv (variantes)                       â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ğŸ¯ SIGUIENTE:                                                               â•‘
â•‘      1. Sube TODOS los submissions a Kaggle                                 â•‘
â•‘      2. Compara los scores pÃºblicos                                         â•‘
â•‘      3. El mejor serÃ¡ tu submission final                                   â•‘
â•‘                                                                              â•‘
â•‘  ğŸ’¡ TIP: Prueba diferentes combinaciones en Kaggle para encontrar           â•‘
â•‘          el mejor peso. El leaderboard pÃºblico te dirÃ¡ cuÃ¡l funciona mejor. â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


if __name__ == "__main__":
    main()